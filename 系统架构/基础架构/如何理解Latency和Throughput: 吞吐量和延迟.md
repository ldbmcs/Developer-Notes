> 转载：[如何理解Latency和Throughput: 吞吐量和延迟](https://www.cnblogs.com/binyao/p/5162424.html)

Latency，中文译作延迟。Throughput，中文译作吞吐量。它们是衡量软件系统的最常见的两个指标。

延迟一般包括单向延迟（One-way Latency）和往返延迟（Round Trip Latency），实际测量时一般取往返延迟。它的单位一般是ms、s、min、h等。

而吞吐量一般指相当一段时间内测量出来的系统单位时间处理的任务数或事务数（TPS）。注意“相当一段时间”，不是几秒，而可能是十几分钟、半个小时、一天、几周甚至几月。它的单位一般是TPS、每单位时间写入磁盘的字节数等。

思考一个问题：

> 低延迟一定意味着高吞吐量吗？如果不是，试举出反例。

假如有一个网站系统，客户端每次请求网站服务端，网络传输时间（包括往返）为 200ms，服务端处理请求为10ms。那么如果是同步请求，则延迟为210ms。此时如果提高网络传输速度，比如提高到100ms，那么延迟为110ms。这种情况减少延迟似乎确实可以一定程度提高吞吐量，原因主要在于：系统性能瓶颈不在于服务端处理速度，而在于网络传输速度。

继续假设将同步请求改为异步请求，那么现在延迟为100ms，延迟降低了，但吞吐量保持不变。所以这是一个反例。

除了上面这个反例外，还有一个更生动的反例：

```
　
火车、飞机运煤：从山西到广州运煤，一列火车100小时（包括往返）可以运输10000t煤，而一架飞机20小时（包括往返）可以运输100t煤
```

显然飞机运煤的延迟明显低于火车运煤，但如果测试运10000t煤，则火车运煤的吞吐量远高于飞机：

- 火车运煤的吞吐量为100t/小时
- 飞机运煤的吞吐量为5t/小时

我们可以将上面的运煤场景类比软件系统，火车、飞机运煤可以比作Web服务器处理请求，比如Apache和Nginx。在并发请求数不高时，比如10000（我假设的）以下时，也许Apache的吞吐量可能优于Nginx，但在大于10000时Apache的吞吐量就开始急剧下降，而Nginx的吞吐量相对之前比较稳定。所以比较Web服务器的吞吐量时，必须观察在并发请求数逐渐递增情况下它们各自的表现。

根据延迟和吞吐量我们还可以计算并发度（Concurrency），公式如下：

```
　　并发度　= 吞吐量 * 延迟
```

比如一个任务的处理花费1ms，吞吐量为1000tps，那么并发度就等于1/1000*1000=1，可以得出任务处理线程模型是单线程模型。

又比如一个HDD磁盘的延迟为8ms，但吞吐量可以达到每秒钟写40MB，那么每次磁盘寻道可以写入的数据量为(40*10^6) * (8*10^-3)B = 320,000B = 320KB。 

下面的比喻是关于[吞吐量](http://www.nowamagic.net/librarys/veda/tag/吞吐量)（throughput）和延迟（latency）的。如果你要搞网络性能优化，这两个概念是你必须要知道的，它们看似简单实则不是。我相信包括我在内的很多人都曾经认为大的吞吐量就意味着低延迟，高延迟就意味着吞吐量变小。下面的比喻可以解释这种观点根本不对。该比喻来自[这里](http://techdiscuss.wordpress.com/2010/01/21/throughput-and-latency/)，我来做个大体意译（非逐字翻译）。

我们可以把网络发送数据包比喻成去街边的 ATM 取钱。每一个人从开始使用 ATM 到取钱结束整个过程都需要一分钟，所以这里的延迟是60秒，那吞吐量呢？当然是 1/60 人/秒。现在银行升级了他们的 ATM 机操作系统，每个人只要30秒就可以完成取款了！延迟是 30秒，吞吐量是 1/30 人/秒。很好理解，可是前面的问题依然存在对不对？别慌，看下面。

因为这附近来取钱的人比较多，现在银行决定在这里增加一台 ATM 机，一共有两台 ATM 机了。现在，一分钟可以让4个人完成取钱了，虽然你去排队取钱时在 ATM 机前还是要用 30 秒！也就是说，延迟没有变，但吞吐量增大了！可见，吞吐量可以不用通过减小延迟来提高。

好了，现在银行为了改进服务又做出了一个新的决定：每个来取钱的客户在取完钱之后必须在旁边填写一个调查问卷，用时也是30秒。那么，现在你去取钱的话从开始使用 ATM 到完成调查问卷离开的时间又是 60 秒了！换句话说，延迟是60秒。而吞吐量根本没变！一分钟之内还是可以进来4个人！可见，延迟增加了，而吞吐量没有变。

从这个比喻中我们可以看出，**延迟测量的是每个客户（每个应用程序）感受到的时间长短，而吞吐量测量的是整个银行（整个操作系统）的处理效率，是两个完全不同的概念**。用作者的原话说是：

> In short, the throughput is a function of how many stages are in parallel while latency is a function of how many are in series when there are multiple stages in the processing. The stage with the lowest throughput determines the overall throughput.

正如银行为了让客户满意不光要提高自身的办事效率外，还要尽量缩短客户在银行办事所花的时间一样，操作系统不光要尽量让网络吞吐量大，而且还要让每个应用程序发送数据的延迟尽量小。这是两个不同的目标。